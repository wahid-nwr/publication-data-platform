version: '3.8'

volumes:
  kafka_data:
  db_data:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
  publication_db_data:
  logs_data:
  csv_data:

networks:
  publication-net:
    external: true

configs:
  fluentd_conf:
    file: ./fluent.conf

secrets:
  gcs_key.json:
    file: /home/wahid/git/gcs-key-files/alert-cursor-476219-s1-43d9dd0a64d2.json
  SEPOLIA_RPC_URL:
    external: true
  WEB3_PRIVATE_KEY:
    external: true
  CONTRACT_ADDRESS_V2:
    external: true
  FIREBASE_SERVICE_ACCOUNT_JSON:
    file: /home/wahid/git/gcs-key-files/firebase-service-account.json

services:
  # Fluentd log collector service
  fluentd:
    image: fluent/fluentd:latest # Or a specific tag like fluent/fluentd-kubernetes-daemonset:v1.7.3-debian-cloudwatch-1.0
    ports:
      - "24224:24224" # Fluentd forward input port
    volumes:
      - ./fluent.conf:/fluentd/etc/fluentd.conf # Mount your config file
      - /var/log/fluentd:/fluentd/log # Persistent storage for Fluentd logs
    deploy:
      mode: global # Run Fluentd on every node
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

#  db:
#    image: postgres:16-alpine
#    container_name: my-postgres-db
#    restart: always
#    ports:
#      - "65432:5432"
#    environment:
#      POSTGRES_DB: publication-db
#      POSTGRES_USER: wahid
#      POSTGRES_PASSWORD: anwar
#    volumes:
#      - db_data:/var/lib/postgresql/data
#    healthcheck:
#      test: ["CMD-SHELL", "pg_isready -U wahid -d publication-db"]
#      interval: 5s
#      timeout: 5s
#      retries: 5
#      start_period: 10s

#  zookeeper:
#    image: confluentinc/cp-zookeeper:7.5.0
#    container_name: zookeeper
#    logging:
#      driver: "none"
#    ports:
#      - "2181:2181"
#    environment:
#      ZOOKEEPER_CLIENT_PORT: 2181
#      ZOOKEEPER_TICK_TIME: 2000
#      ZOOKEEPER_LOG4J_LOGGERS: "org.apache.zookeeper=ERROR"
#    healthcheck:
#      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
#      interval: 10s
#      timeout: 5s
#      retries: 5
#    volumes:
#      - logs_data:/zookeeper/logs

#  kafka:
#    image: confluentinc/cp-kafka:7.5.0
#    container_name: kafka
#    logging:
#      driver: "none"
#    ports:
#      - "29092:9092"
#    environment:
#      KAFKA_BROKER_ID: 1
#      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
#      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
#      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
#      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#      KAFKA_LOG4J_LOGGERS: "kafka=ERROR,kafka.controller=ERROR,state.change.logger=ERROR"
#      LOG_LEVEL: "ERROR"
#    depends_on:
#      - zookeeper
#    healthcheck:
#      test: ["CMD", "bash", "-c", "nc -z localhost 9092"]
#      interval: 10s
#      timeout: 5s
#      retries: 10
#    volumes:
#      - kafka_data:/kafka/logs
  csv-producer:
    image: wahid/csv-producer:latest
    command: [ "--log.level=error", "--log.format=json" ]
    #    depends_on:
    #      - db
    ports:
      - "8082:8082"
    networks:
      - publication-net
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 10
        window: 120s
    environment:
      WEB_PORT: "8082"
      GOOGLE_APPLICATION_CREDENTIALS: "/run/secrets/gcs_key.json"
      AUTHOR_CSV_PATH: "gs://publication-csv-bucket/autoren.csv"
      AUTHOR_CSV_CHARSET: "ISO_8859_1"
      BOOK_CSV_PATH: "gs://publication-csv-bucket/buecher.csv"
      BOOK_CSV_CHARSET: "windows-1252"
      MAGAZINE_CSV_PATH: "gs://publication-csv-bucket/zeitschriften.csv"
      MAGAZINE_CSV_CHARSET: "ISO_8859_1"
      fromVolume: "false"
      KAFKA_BOOTSTRAP_SERVERS: "pkc-lgk0v.us-west1.gcp.confluent.cloud:9092"
      KAFKA_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required username='DRVLONONKVQ46SJK' password='cfltRl737pTCUGonN6hUMpR8+a5uKaVRxUnNjf8F538KPAKRZpB8TnA/QxWej6Cg';"
      KAFKA_SECURITY_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM: PLAIN
      LOG_LEVEL: "ERROR"
      DB_HOST: "pubdb"
      DB_PORT: "5432"
    secrets:
      - gcs_key.json
      - SEPOLIA_RPC_URL
      - WEB3_PRIVATE_KEY
      - CONTRACT_ADDRESS_V2
    volumes:
      # Absolute path for CSVs on host
      - /home/wahid/git/publication_data:/csv/data:ro
      - logs_data:/producer/logs

  app:
    build:
      context: .
      args:
        MY_DB_HOST: "pubdb"
        MY_DB_PORT: "5432"
    image: wahid/publication-persister:latest
#    depends_on:
#      - db
#    ports:
#      - "8081:8081"
#      - "9494:9494"
    ports:
      - target: 8081
        published: 8081
        protocol: tcp
        mode: host
      - target: 9494
        published: 9494
        protocol: tcp
        mode: host
    networks:
      - publication-net
#    logging:
#      driver: loki
#      options:
#        loki-url: https://logs-prod-021.grafana.net/loki/api/v1/push
#        loki-retries: "5"
#        loki-batch-size: "400"
#        loki-min-backoff: "500ms"
#        loki-max-backoff: "5s"
#        loki-external-labels: "job=publication_app,instance=pubstack_app"
#        loki-basic-auth-username: "username"        # fixed value for Grafana Cloud
#        loki-basic-auth-password: "glc_.................................."
    deploy:
#      replicas: 1
#      placement:
#        constraints: [ node.role == worker ]
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 10
        window: 120s
      resources:
        limits:
          cpus: '0.2'
          memory: 768M
        reservations:
          cpus: '0.1'
          memory: 512M
#    healthcheck:
#      test: ["CMD-SHELL", "curl -f http://localhost:8088/health || exit 1"]
#      interval: 10s
#      timeout: 5s
#      retries: 6
#      start_period: 20s
    environment:
      WEB_PORT: "8081"
      KAFKA_BOOTSTRAP_SERVERS: "pkc-lgk0v.us-west1.gcp.confluent.cloud:9092"
      KAFKA_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required username='DRVLONONKVQ46SJK' password='cfltRl737pTCUGonN6hUMpR8+a5uKaVRxUnNjf8F538KPAKRZpB8TnA/QxWej6Cg';"
      KAFKA_SECURITY_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM: PLAIN
      ZOOKEEPER_HOST: "zookeeper:2181"
      LOG_LEVEL: "ERROR"
      OTEL_SERVICE_NAME: publication-app
      OTEL_EXPORTER_OTLP_ENDPOINT: http://observability_grafana-alloy:4318
      OTEL_EXPORTER_OTLP_PROTOCOL: http/protobuf
      OTEL_RESOURCE_ATTRIBUTES: deployment.environment=local,service.version=1.0.0
      DB_HOST: "pubdb"
      DB_PORT: "5432"
      GOOGLE_APPLICATION_CREDENTIALS: /run/secrets/FIREBASE_SERVICE_ACCOUNT_JSON
    volumes:
      - publication_db_data:/app/data
      - logs_data:/app/logs
    secrets:
      - SEPOLIA_RPC_URL
      - WEB3_PRIVATE_KEY
      - CONTRACT_ADDRESS_V2
      - FIREBASE_SERVICE_ACCOUNT_JSON

#  author-writer:
#    image: wahid/author-writer:latest
#    depends_on:
#      - kafka
#      - db
#      - app
#    command: ["java", "-jar", "app.jar"]
#    environment:
#      - LOG_LEVEL=debug
#    volumes:
#      - logs_data:/author-writer/logs
#
#  publication-writer:
#    image: wahid/publication-writer:latest
#    depends_on:
#      - kafka
#      - db
#      - app
#    command: ["java", "-jar", "app.jar"]
#    environment:
#      - LOG_LEVEL=debug
#    volumes:
#      - logs_data:/pub-writer/logs

#  publication-orchestrator:
#    image: wahid/publication-orchestrator:latest
#    depends_on:
#      - kafka
#      - db
#      - app
#    command: ["java", "-jar", "app.jar"]
#    environment:
#      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
#      ZOOKEEPER_HOST: "zookeeper:2181"
#      LOG_LEVEL: "debug"
#    volumes:
#      - publication_db_data:/app/data
#      - logs_data:/orchestrator/logs
