#version: '3.8'

volumes:
  kafka_data:
    driver_opts:
      type: tmpfs
      device: tmpfs
  db_data:
  publication-db-data:
  publications_data:
  # Optional: You can add driver or driver_opts here if you need
  # a specific volume driver (e.g., for cloud storage).
   driver: local
   driver_opts:
     type: nfs
     o: addr=192.168.1.1,rw
     device: ":/tmp/publication/data"

services:
  db:
    image: postgres:16-alpine # Use a lightweight PostgreSQL image
    container_name: my-postgres-db
    restart: always
    ports:
      - "65432:5432" # Map host port 5432 to container port 5432
    environment:
      POSTGRES_DB: publication-db
      POSTGRES_USER: wahid
      POSTGRES_PASSWORD: anwar
    volumes:
      - db_data:/var/lib/postgresql/data # Persist data in a named volume
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U wahid -d publication-db"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

#  h2db:
#    image: oscarfonts/h2
#    ports:
#      - "8082:81"    # web console accessible at http://localhost:8082
#      - "1521:1521"  # JDBC TCP port
#    environment:
#      - H2_OPTIONS=-tcp -tcpAllowOthers -web -webAllowOthers -tcpPort 1521 -ifNotExists
#    volumes:
#      - publication-db-data:/opt/h2-data
#    healthcheck:
#      test: [ "CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/1521' || exit 1" ]
#      interval: 5s
#      timeout: 3s
#      retries: 10
#      start_period: 10s

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    logging:
      driver: "none"   # completely hide logs
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_LOG4J_LOGGERS: "org.apache.zookeeper=ERROR"
    healthcheck:
      test: [ "CMD", "echo", "ruok", "|", "nc", "localhost", "2181" ]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - ./logs:/zookeeper/logs

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    logging:
      driver: "none"   # completely hide logs
    ports:
      - "29092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # define both listeners
#      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG4J_LOGGERS: "kafka=ERROR,kafka.controller=ERROR,state.change.logger=ERROR"
      LOG_LEVEL: "debug"
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "bash", "-c", "nc -z localhost 9092" ]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - ./logs:/kafka/logs

  csv-producer:
    build: ./csv-producer
    depends_on:
      kafka:
        condition: service_healthy
    command: ["java", "-jar", "app.jar"]
    environment:
      AUTHOR_CSV_PATH: "/app/data/5k_large.csv"
      AUTHOR_CSV_CHARSET: "ISO_8859_1"
      BOOK_CSV_PATH: "/app/data/buecher.csv"
      BOOK_CSV_CHARSET: "windows-1252"
      MAGAZINE_CSV_PATH: "/app/data/zeitschriften.csv"
      MAGAZINE_CSV_CHARSET: "ISO_8859_1"
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      LOG_LEVEL: "debug"
    volumes:
      - publication-db-data:/app/data
      - ./csv-producer/src/main/resources/data:/app/data
      - ./logs:/producer/logs

#  author-writer:
#    build: ./author-writer
#    depends_on:
#      kafka:
#        condition: service_healthy
#      db:
#        condition: service_healthy
#      app:
#        condition: service_healthy
#    command: ["java", "-jar", "app.jar"]
#    environment:
#      - LOG_LEVEL=debug
#    volumes:
#      - ./logs:/author-writer/logs
#
#  publication-writer:
#    build: ./publication-writer
#    depends_on:
#      kafka:
#        condition: service_healthy
#      db:
#        condition: service_healthy
#      app:
#        condition: service_healthy
#    command: ["java", "-jar", "app.jar"]
#    environment:
#      - LOG_LEVEL=debug
#    volumes:
#      - ./logs:/pub-writer/logs

  publication-orchestrator:
    build: deprecated/publication-orchestrator
    depends_on:
      app:
        condition: service_healthy
      kafka:
        condition: service_healthy
      db:
        condition: service_healthy
    command: [ "java", "-jar", "app.jar" ]
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      ZOOKEEPER_HOST: "zookeeper:2181"
      LOG_LEVEL: "debug"
    volumes:
      - publication-db-data:/app/data
      - ./logs:/orchestrator/logs

  app:
    build: ./publication-common   # <-- path where your Java app Dockerfile lives
    container_name: csv-app
    depends_on:
      kafka:
        condition: service_healthy
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 20s
#      test: [ "CMD-SHELL", "java -cp /app/app.jar io.wahid.publication.DBHealthCheck || exit 1" ]
#      interval: 10s
#      timeout: 5s
#      retries: 6
#      start_period: 20s
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      ZOOKEEPER_HOST: "zookeeper:2181"
      LOG_LEVEL: "debug"
    volumes:
      - publication-db-data:/app/data
      - ./data:/app/data   # optional: mount your CSVs
      - ./logs:/app/logs
